---
author: Pacific2021
categories:
- Machine Learning
- Python
date: "2023-02-22"
description: 
image: 
math: true
tags:
- Machine Learning
- Python
- ensemble
- stacking
- chatGPT
title: é›†æˆå­¸ç¿’æ¡†æ¶ï¼šstacking
---



## ensemble model æ®ºå™¨ï¼šstaking å †ç–Šæ¨¡å‹



stacking æ˜¯é›†æˆï¼ˆensembleï¼‰æ¨¡å‹çš„æ–¹æ³•ï¼Œä¹Ÿå¯ä»¥è¦–ç‚ºæå–ç‰¹å¾µçš„æ–¹å¼ã€‚ä¸»è¦æ˜¯ç‚ºäº†é™ä½æ–¹å·®ï¼ˆvarianceï¼‰ï¼Œä½†å¤šå±¤çš„æƒ…æ³ä¸‹ä¹Ÿèƒ½é™ä½åå·®ï¼ˆdeviationï¼‰ã€‚

å †ç–Šæ¨¡å‹çš„æ–¹æ³•åœ¨è³‡æ–™ç«¶è³½å¹³å° `Kaggle` ä¸­æ™‚å¸¸æ˜¯è´å¾—æ¯”è³½çš„ç­–ç•¥ï¼Œå †ç–Šæ¨¡å‹çš„æ¦‚å¿µæ˜¯é€šéå¤šå€‹æ¨¡å‹ä½¿ç”¨ `out- of-fold`çš„æ–¹æ–¹å¼ä¾†è¨“ç·´ä¾¿å°‡é æ¸¬å€¼è¦–ç‚ºæ–°ç‰¹å¾µï¼Œé€™å€‹æ–¹å¼å¯ä»¥å¼·åŒ–æ¯å€‹ model çš„å¼±é»ï¼Œç°¡å–®ä¾†èªªåˆ©ç”¨ä¸åŒæ¼”ç®—æ³•å¾ä¸åŒçš„æ•¸æ“šç©ºé–“è§’åº¦å’Œæ•¸æ“šçµæ§‹è§’åº¦çš„å°æ•¸æ“šçš„ä¸åŒè§€æ¸¬ï¼Œä¾†å–é•·è£œçŸ­ï¼Œå„ªåŒ–çµæœã€‚å› æ­¤æœƒå»ºè­°ç”¨åŸç†æˆ–è¨ˆç®—æ–¹å¼è¼ƒç‚ºä¸åŒçš„ model ä¾†åšç‚ºè¨“ç·´ modelã€‚

> é€™äº›æ¨¡å‹æˆ‘å€‘ç¨±ç‚º meta learnerï¼Œç”¢ç”Ÿå‡ºä¾†çš„ç‰¹å¾µå‰‡ç¨±ç‚º meta feature

èˆ‰å€‹ä¾‹å­ï¼ŒKNN æ˜¯é€éè·é›¢çš„é è¿‘ä¾†åˆ†é¡ï¼Œæ±ºç­–æ•¸å‰‡æ˜¯é€é `entropy æˆ– Gini` ä¸ç´”åº¦ä¾†åˆ†è£‚ç¯€é»ï¼Œä¸¦æ‰¾å‡ºåˆ†é¡è¦å‰‡ï¼Œè€Œ  Random Forest å‰‡æ˜¯é€é bagging å¾ŒæŠ•ç¥¨ä¾†åšå‡ºé æ¸¬ï¼Œå› ä½¿ä¸åŒè§€é»çš„æ¨¡å‹å’Œæ¼”ç®—æ³•ï¼Œéƒ½æœ‰ä¸ä¸€æ¨£çš„å„ªç¼ºé»ï¼Œæ•…ä½¿ç”¨ stacking å°±æ˜¯å°‡å„å€‹æ¨¡å‹çš„å„ªç¼ºé»æˆªé•·è£œçŸ­ã€‚



### æ­¥é©Ÿä¸€ | å»ºç«‹ base learner



é¦–å…ˆå…ˆå°å…¥ data, ä¸¦å°‡ data åˆ†ç‚º train & testï¼Œå› ç‚ºè¦ä½¿ç”¨ out-of-fold ä¾†å †ç–Šä¸åŒçš„æ¨¡å‹ï¼Œæ•…é ˆå°‡ train data å†åˆ†ç‚º train data å’Œ valid data



é€™é‚Šæ˜¯ç”¨ä¹³ç™Œçš„è³‡æ–™ä¾†åšç¤ºç¯„

```python
from sklearn.datasets import load_breast_cancer
from sklearn.metrics import log_loss
from sklearn.model_selection import KFold
import numpy as np
import pandas as pd
import os

# è¼‰å…¥ä¹³ç™Œè³‡æ–™é›†
breast_cancer = load_breast_cancer()

train_x, test_x, train_y, test_y = train_test_split(
    breast_cancer.data, breast_cancer.target, test_size = 0.2, random_state = 0)

train_x = pd.DataFrame(train_x, columns=['feature_{}'.format(i) for i in range(train_x.shape[1])])
train_y = pd.Series(train_y)
test_x = pd.DataFrame(test_x, columns=['feature_{}'.format(i) for i in range(test_x.shape[1])])
```



å…ˆå„è‡ªå®šç¾©ç¬¬ä¸€å±¤æ¨¡å‹ï¼Œç¬¬ä¸€å±¤æ¨¡å‹ç›¡é‡ä½¿ç”¨ä¸åŒåŸç†çš„ model ä¾†å †ç–Šï¼Œæ•…é€™é‚Šåˆ†åˆ¥ä½¿ç”¨ tree-baesd ä¸¦ä½¿ç”¨ begging æŠ•ç¥¨æ–¹å¼çš„ Random Forestï¼Œä½¿ç”¨é«˜ç¶­ç©ºé–“çš„å¹³é¢ä¸¦å°‹æ‰¾é‚Šç•Œæœ€å¤§åŒ–çš„ SVMï¼Œä»¥åŠ GBDT å®¶æ—ä¸­çš„ XGBoost

> é€™é‚Šç¨å¾®èªªæ˜ä¸€ä¸‹ï¼Œå› ç‚º sklearn ä¸­æ¨¡å‹è¨ˆç®—æ˜¯ä½¿ç”¨ numpy arrayï¼Œæ•… array ä¸­åœ¨è¨“ç·´æ¨¡å‹æ˜¯ä¸èƒ½æœ‰ NA å€¼çš„ï¼Œä¸ç„¶ä¸€èˆ¬ä¾†èªª tree-based çš„ modelï¼Œä¾‹å¦‚ Random Forest æ˜¯ä¸éœ€è¦å¡«è£œ NA å€¼çš„

> é€™é‚Šçš„ parms ä¸¦æ²’æœ‰ç‰¹åˆ¥å„ªåŒ–ï¼Œåªæ˜¯ä¾ç…§ä¸€èˆ¬æœƒä½¿ç”¨çš„å¤§ç•¥å€¼ä¾†è¨ˆç®—



```python
# SVM
class SVM:
    def __init__(self):
        self.model = None
        self.scaler = None
        self.imputer = None

    def fit(self, tr_x, tr_y, va_x, va_y):
        self.imputer = SimpleImputer(strategy = 'mean')
        tr_x = self.imputer.fit_transform(tr_x)
        self.scaler = StandardScaler()
        self.scaler.fit(tr_x)
        tr_x = self.scaler.transform(tr_x)
        self.model = SVC(C = 1.0, kernel = 'rbf', gamma ='auto', probability = True)
        self.model.fit(tr_x, tr_y)

    def predict(self, x):
        x = self.imputer.transform(x)
        x = self.scaler.transform(x)
        pred = self.model.predict_proba(x)[:, 1]
        return pred

# Random Forest
    class RandomForest:
        def __init__(self):
            self.model = None
            self.scaler = StandardScaler()
            self.imputer = SimpleImputer(strategy = 'mean')

        def fit(self, tr_x, tr_y, va_x, va_y):
            tr_x = self.imputer.fit_transform(tr_x)
            params = {'criterion': 'entropy', 'max_depth': 5, 'random_state': 71}
            n_estimators = 100
            self.scaler.fit(tr_x)
            tr_x = self.scaler.transform(tr_x)
            self.model = RandomForestClassifier(n_estimators = n_estimators, **params)
            self.model.fit(tr_x, tr_y)

        def predict(self, x):
            x = self.imputer.transform(x)
            x = self.scaler.transform(x)
            pred = self.model.predict_proba(x)[:, 1]
            return pred

# XGBoost
    class Model1Xgb:

        def __init__(self):
            self.model = None

        def fit(self, tr_x, tr_y, va_x, va_y):
            params = {'objective': 'binary:logistic', 'silent': 1, 'random_state': 71,
                      'eval_metric': 'logloss'}
            num_round = 20
            dtrain = xgb.DMatrix(tr_x, label=tr_y)
            dvalid = xgb.DMatrix(va_x, label=va_y)
            watchlist = [(dtrain, 'train'), (dvalid, 'eval')]
            self.model = xgb.train(params, dtrain, num_round, evals = watchlist)

        def predict(self, x):
            data = xgb.DMatrix(x)
            pred = self.model.predict(data)
            return pred
```



å†ä¾†æ˜¯å®šç¾© stacking çš„æ¶æ§‹ï¼Œå…¶å¯¦ç›®å‰æ˜¯æœ‰ stcking çš„å¥—ä»¶ä¾†ç›´æ¥å¹«å¿™åš stacking å †ç–Šï¼Œä½† stacing å †ç–Šçš„æ–¹å¼è¼ƒç‚ºè¤‡é›œï¼Œé€éè‡ªè¨‚ç¾©ä¾†å¹«åŠ©äº†è§£æ•´å€‹æµç¨‹æ˜¯æ»¿æœ‰å¹«åŠ©çš„ã€‚



```python
def predict_cv(model, train_x, train_y, test_x):
    preds = []
    preds_test = []
    va_idxes = []
    x = []
    kf = KFold(n_splits = 4, shuffle = True, random_state = 71)

    # åœ¨äº¤å‰é©—è­‰ä¸­é€²è¡Œè¨“ç·´/é æ¸¬ï¼Œä¸¦ä¿å­˜é æ¸¬å€¼åŠç´¢å¼•
    for i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):
        tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx] # kfold ç”¢ç”Ÿè¨“ç·´é›†å’Œé©—è­‰é›†
        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]
        model.fit(tr_x, tr_y, va_x, va_y)
        pred = model.predict(va_x)
        preds.append(pred)
        pred_test = model.predict(test_x)
        preds_test.append(pred_test)
        va_idxes.append(va_idx)

    # å°‡é©—è­‰è³‡æ–™çš„é æ¸¬å€¼æ•´åˆèµ·ä¾†ï¼Œä¸¦ä¾åºæ’åˆ—
    va_idxes = np.concatenate(va_idxes)
    preds = np.concatenate(preds, axis=0)
    order = np.argsort(va_idxes)
    pred_train = preds[order]

    # å–æ¸¬è©¦è³‡æ–™çš„é æ¸¬å€¼å¹³å‡
    preds_test = np.mean(preds_test, axis=0)

    return pred_train, preds_test
```



å¯ä»¥å¾ç¨‹å¼ç¢¼ä¸­ä¾†ä¸€çªºæ•´å€‹ stacking çš„é‹ä½œæ¶æ§‹ï¼Œé€™è£¡å°‡ base learner åˆ†ç‚ºå¹¾å€‹æµç¨‹çš„æ­¥é©Ÿä¾†çœ‹ã€‚

1. åˆ†å‰²è³‡æ–™é›†ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†
2. å®šç¾© base leaner éœ€è¦çš„ model
3. ä½¿ç”¨ `k-fold äº¤å‰é©—è­‰`ä¾†è¨“ç·´åŸºæœ¬æ¨¡å‹



1 å’Œ 2 ä¸Šé¢å·²ç¶“å®Œæˆäº†ï¼Œæ¯”è¼ƒè¤‡é›œçš„å°±æ˜¯é€é k-fold ä¾†è¨“ç·´æ¨¡å‹ï¼Œå¯ä»¥å¾ä¸‹åœ–ä¾†çœ‹ä¸€ä¸‹æ•´å€‹ stakcing çš„æ¶æ§‹ã€‚



![v2-911debe2380e245eb3ee70b72b0e7d08_1440w](https://cdn.jsdelivr.net/gh/PacificLin/chart-bed@main/blogv2-911debe2380e245eb3ee70b72b0e7d08_1440w.png)



çœ‹èµ·ä¾†éå¸¸çš„è¤‡é›œï¼Œä½†æˆ‘å€‘å¯ä»¥ä¸€ä¸€çš„ä¾†æ‹†è§£ï¼Œæˆ‘å€‘çš„ base leaner ç”¨ä¸‰å€‹æ¨¡å‹ä¾†å»ºæ§‹ï¼Œåˆ†åˆ¥ç‚º SVMã€XGBoostã€randomforstã€‚ä¸‰å€‹ä¸€èµ·çœ‹æœ‰é»è¤‡é›œï¼Œé‚£å…ˆæ‹†è§£ç‚ºå…¶ä¸­ä¸€å€‹æ¨¡å‹çš„è¨“ç·´éç¨‹å¦‚ä¸‹



![image-20230509172540811](https://cdn.jsdelivr.net/gh/PacificLin/chart-bed@main/blogimage-20230509172540811.png)



å‡è¨­æˆ‘å€‘å…¶ä¸­ä¸€å€‹ model SVM é–‹å§‹è¨“ç·´ï¼Œä¾åœ–ä¾†çœ‹ï¼Œå°±æ˜¯å…ˆå°‡ train data åˆ†æˆ n æŠ˜ï¼Œå› æ­¤æ¯ä¸€æŠ˜çš„ data row çš„æ•¸é‡å°±éƒ½æ˜¯ 1/nï¼Œè€Œç¬¬ä¸€æ¬¡è¨“ç·´ç”¨å…¶ä»–æŠ˜ä¾†åšè¨“ç·´è³‡æ–™ã€‚

> train data æ­¤æ™‚åˆ†ç‚º train data å’Œ validate data

train å®Œå¾Œçš„ model åœ¨å° validate data åš predict æ­¤æ™‚å°±ç”¢ç”Ÿäº† 1/n è¡Œçš„é æ¸¬è³‡æ–™ï¼Œé€™å°±æ˜¯æˆ‘å€‘çš„æ–°ç‰¹å¾µã€‚

è€ŒåŒæ™‚è¨“ç·´å®Œçš„è³‡æ–™ä¹Ÿå° test data åš predict ï¼Œç•¶ç„¶ä¹Ÿç”¢ç”Ÿäº†æ–°çš„é æ¸¬è³‡æ–™



## é‡è¤‡ K æ¬¡ï¼ˆk-fold å¹¾æŠ˜å°±å¹¾æ¬¡ï¼‰

ç„¶å¾Œå°±æœƒç”¢ç”Ÿå¦‚ä¸‹åœ–çš„çµæœï¼Œå‡è¨­æˆ‘å€‘åˆ†ç‚º 5 æŠ˜ï¼Œé‚£æ¯è¨“ç·´ä¸€æ¬¡å°±æœƒç”¢ç”Ÿ 1/5 è¡ŒåŸå§‹ train data çš„è³‡æ–™ï¼Œæœ€å¾Œåˆ†åˆ¥æŠ˜å®Œ 5 æ¬¡å’Œè¨“ç·´ 5 æ¬¡å¾Œï¼Œå°±ç”¢ç”Ÿäº†å’ŒåŸå§‹ train data è¡Œæ•¸çš„è³‡æ–™ç‰¹å¾µã€‚



![image-20230509172906477](https://cdn.jsdelivr.net/gh/PacificLin/chart-bed@main/blogimage-20230509172906477.png)



test data åŒæ™‚æ¯æ¬¡è¨“ç·´ä¹Ÿéƒ½æœ‰ç”¢ç”Ÿæ–°çš„ predictionï¼Œå› æ­¤æœ€å¾Œæœƒå°‡ test data çš„ prediction å–å¹³å‡å¾Œç”¢ç”Ÿæ–°çš„ test data

> ä¸ä¸€å®šè¦ç”¨å¹³å‡ï¼Œå¯ä»¥ç”¨å…¶ä»–æ–¹è™•ç†

å› æ­¤æœ€å¾Œæœƒç”¢ç”Ÿå‡ºå’ŒåŸæœ‰ train data ä¸€æ¨£è¡Œæ•¸ï¼Œä½†åˆ—æ•¸ç‚º model æ•¸é‡çš„ data frameï¼Œé€™äº›å°±æ˜¯æ–°çš„ç¬¬äºŒå±¤ featureï¼Œè€Œæ¥è‘— meta model å°±è¦ç™»å ´äº†

> é€™è£¡çš„ test data å¯ä»¥æƒ³åƒç‚ºå°å…¶åš stacking çš„ transform



![image-20230509170943466](https://cdn.jsdelivr.net/gh/PacificLin/chart-bed@main/blogimage-20230509170943466.png)



### æ­¥é©ŸäºŒ | è¨“ç·´ meta model

ä¸€èˆ¬ä¾†èªª stcking model å¾ˆå®¹æ˜“å°±éæ“¬åˆï¼Œå› æ­¤åœ¨æœ€å¾Œä¸€å±¤ç”¢ç”Ÿæœ€çµ‚è³‡æ–™çµæœçš„ model éƒ½ä¸æœƒä½¿ç”¨å¤ªè¤‡é›œçš„æ¨¡å‹ï¼Œä¸€èˆ¬å¯ä»¥æ¨è–¦ç¸£æ€§æ¨¡å‹ç›¸é—œï¼Œé€™é‚Šå°±æ˜¯ä½¿ç”¨ logistic model

```python
# å»ºç«‹ç·šæ€§æ¨¡å‹
class Model2Linear:

    def __init__(self):
        self.model = None
        self.scaler = None

    def fit(self, tr_x, tr_y, va_x, va_y):
        self.scaler = StandardScaler()
        self.scaler.fit(tr_x)
        tr_x = self.scaler.transform(tr_x)
        self.model = LogisticRegression(solver='lbfgs', C=1.0)
        self.model.fit(tr_x, tr_y)

    def predict(self, x):
        x = self.scaler.transform(x)
        pred = self.model.predict_proba(x)[:, 1]
        return pred
```



æœ€å¾Œä¸€æ­¥å°±æ˜¯è¨“ç·´ meta modelï¼Œè€Œ meta model å°±æ˜¯æ‹¿æ–°ç”¢ç”Ÿçš„ç‰¹å¾µå€¼ç•¶ä½œ Xï¼Œ ç„¶å¾ŒåŸæœ¬ train data çš„ label å€¼æ‹¿ä¾†ç•¶ y ä¾†è¨“ç·´ meta modelã€‚



![image-20230509173134501](https://cdn.jsdelivr.net/gh/PacificLin/chart-bed@main/blogimage-20230509173134501.png)

> é€™æ™‚å€™å¯ä»¥è€ƒæ…®åŠ ä¸ŠåŸæœ‰çš„ç‰¹å¾µåŠ å…¥è¨“ç·´ï¼Œä¸éé€™æ¨£æ¯”è¼ƒå®¹æ˜“éæ“¬åˆï¼Œæˆ–æ˜¯å°‡åŸæœ‰çš„ç‰¹å¾µåšé™ç¶­å¾Œå†åŠ å…¥è¨“ç·´ã€‚

test data å¯ä»¥é€éå¤šæŠ˜å¾Œé æ¸¬çš„çš„å¹³å‡ä¾†é æ¸¬ test dataï¼Œä¹Ÿå¯ä»¥å°‡åœ¨æŠ˜å¾Œç”¢ç”Ÿ train data çš„æ–°ç‰¹å¾µå¾Œï¼Œé‡æ–°ç”¨å…¨éƒ¨çš„ train data å° model åš training ç„¶å¾Œåœ¨å° test data åšå‡ºé æ¸¬ã€‚

åœ¨ç¬¬äºŒå±¤æœ‰ä¸€äº›å°è®ŠåŒ–å¯ä»¥èª¿æ•´ï¼Œä¸éé‡è¦çš„æ˜¯éœ€è¦ä¾†è‡ªä¸Šä¸€å±¤çš„æ¨¡å‹åœ¨æœªçŸ¥æ¨™ç±¤ä¸‹å»åšå‡ºé æ¸¬ã€‚

ä»¥ä¸‹å°±æ˜¯é‡å°å„å€‹  base leaner ä¸­çš„ model åš predict



```python
# ä½¿ç”¨ XGBoost
import xgboost as xgb
model_1a = Model1Xgb()
pred_train_1a, pred_test_1a = predict_cv(model_1a, train_x, train_y, test_x)

# ä½¿ç”¨ SVM
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
model_1b = SVM()
pred_train_1b, pred_test_1b = predict_cv(model_1b, train_x, train_y, test_x)

# ä½¿ç”¨ RandomForest
from sklearn.ensemble import RandomForestClassifier
model_1c = RandomForest()
pred_train_1c, pred_test_1c = predict_cv(model_1c, train_x, train_y, test_x)
```



ç„¶å¾Œå°‡å„å€‹ base model è¨ˆç®—å‡ºçš„çµæœå°æ¯” train data çš„ y çœ‹ logloss çš„æ•ˆæœï¼Œä¸éé€™æ˜¯åœ¨ train data ä¸Šçš„è©•åƒ¹ï¼Œä¸ä»£è¡¨çœŸæ­£å°æ–¼æœªçŸ¥ data çš„é æ¸¬ç‹€æ³



```python
print('logloss: {:.4f}'.format(log_loss(train_y, pred_train_1a, eps = 1e-7)))
print('logloss: {:.4f}'.format(log_loss(train_y, pred_train_1b, eps = 1e-7)))
print('logloss: {:.4f}'.format(log_loss(train_y, pred_train_1c, eps = 1e-7)))
```

å¯ä»¥çœ‹åˆ°çµæœï¼Œæ„å¤–çš„ç«Ÿç„¶æ˜¯ SVM æ•ˆæœæœ€å¥½

```python
logloss: 0.1553
logloss: 0.0989
logloss: 0.1195
```



æ¥è‘—æˆ‘å€‘è¦ä¾†è¨“ç·´ meta modelï¼Œé¦–å…ˆå…ˆå°‡æ–°çš„ç‰¹å¾µå€¼åˆä½µ

```python
# å°‡é æ¸¬å€¼ä½œç‚ºç‰¹å¾µä¸¦å»ºç«‹ dataframe
train_x_2 = pd.DataFrame({'pred_1a': pred_train_1a, 'pred_1b': pred_train_1b, 'pred_1c': pred_train_1c})
test_x_2 = pd.DataFrame({'pred_1a': pred_test_1a, 'pred_1b': pred_test_1b, 'pred_1c': pred_test_1c})
```

ç„¶å¾Œå®šç¾© logistic model

```python
# å»ºç«‹ç·šæ€§æ¨¡å‹
class Model2Linear:

    def __init__(self):
        self.model = None
        self.scaler = None

    def fit(self, tr_x, tr_y, va_x, va_y):
        self.scaler = StandardScaler()
        self.scaler.fit(tr_x)
        tr_x = self.scaler.transform(tr_x)
        self.model = LogisticRegression(solver='lbfgs', C=1.0)
        self.model.fit(tr_x, tr_y)

    def predict(self, x):
        x = self.scaler.transform(x)
        pred = self.model.predict_proba(x)[:, 1]
        return pred
```



æœ€å¾Œ fit ä»¥ meta model ä¾† fit æ–°çš„ç‰¹å¾µå€¼ï¼Œé€™é‚Šå› ç‚ºå·²ç¶“å®šç¾©äº† predict_cv çš„æ–¹æ³•ï¼Œæ•…ä»ç”¨ oof ä¾†è¨“ç·´ï¼Œä¸éç¬¬äºŒå±¤å¯ä»¥ç›´æ¥è¨“ç·´ä¸ç”¨ä¸€å®šè¦ä½¿ç”¨æŠ˜å¤–

```python
model_2 = Model2Linear()

pred_train_2, pred_test_2 = predict_cv(model_2, train_x_2, train_y, test_x_2)
print(f'logloss: {log_loss(train_y, pred_train_2, eps=1e-7):.4f}')
```

æœ€å¾Œçœ‹ä¸€ä¸‹å…¶ loglossï¼Œå¯ä»¥çœ‹å‡ºç›¸è¼ƒæ–¼ 3 å„ base-model å…¶ logloss åˆå†ä¸‹é™æƒ¹ã€‚

```python
logloss: 0.0898
```



---



## è©¦è‘—ä½¿ç”¨ chatGPT å¯«ç¨‹å¼

å› ç‚ºåœ¨ç†è§£ stacking çš„éç¨‹ä¸­ï¼Œç¶²è·¯ä¸Šè³‡æ–™ä¸å¤šï¼ŒKaggle ä¸Šçš„æ•™å­¸æœ‰äº› code çœŸçš„å¤ªè¤‡é›œçœ‹ä¸å¤ªæ‡‚ï¼ŒèŠ±æ»¿å¤šæ™‚é–“æ‰çœŸçš„äº†è§£å…¶æ¶æ§‹ï¼Œå¾Œä¾†æœ‰ä½¿ç”¨ chatGPT ä¾†åšä¸€äº›è©¢å•ï¼Œåœ¨äº¤è«‡å’Œä¿®æ­£çš„éç¨‹ï¼Œä¹Ÿé€é chatGPT å®Œæˆä¸€æ®µç®—æ˜¯å®Œæ•´çš„ stacking flow codeï¼Œæˆ‘å€‘å¯ä»¥çœ‹ä¸€ä¸‹ï¼Œé€é chatGPT å¯«å‡ºä¾†çš„ç¨‹å¼å’Œæˆ‘å€‘åŸæœ¬çš„æœ‰å•¥å·®ç•°ã€‚é€™é‚ŠåŒæ¨£éƒ½æ˜¯ç”¨ä¹³ç™Œè³‡æ–™ä¾†å»ºç«‹ stacking flowã€‚

```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
import numpy as np
```

```python
# è¼‰å…¥ä¹³ç™Œè³‡æ–™é›†
breast_cancer = load_breast_cancer()

# åˆ†å‰²è³‡æ–™é›†ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†
X_train, X_test, y_train, y_test = train_test_split(
    breast_cancer.data, breast_cancer.target, test_size=0.2, random_state=0)
```



é€™é‚Šæ¯”è¼ƒç°¡å–®çš„å»ºç«‹ base model å’Œ meta modelï¼Œç”šè‡³æ²’æœ‰å•¥åšè¶…åƒæ•¸è¨­å®šï¼Œä¸éå¦‚éé€²ä¸€æ­¥ç´°ç·»çš„å»è¦æ±‚ chatGPT å…¶å¯¦ä»–æœƒå®Œæˆçš„æ›´å®Œæ•´ã€‚ğŸ™„



```python
# å®šç¾©åŸºæœ¬æ¨¡å‹
base_models = [
    RandomForestClassifier(n_estimators = 100, random_state = 0),
    XGBClassifier(n_estimators = 100, random_state = 0),
    SVC(random_state = 0)
]

# å®šç¾© meta-model
meta_model = LogisticRegression(random_state = 0)
```

```python
# å®šç¾©äº¤å‰é©—è­‰çš„ fold æ•¸é‡
n_folds = 5

# åˆå§‹åŒ– arraysï¼Œç”¨ä¾†ä¿å­˜åŸºæœ¬æ¨¡å‹å’Œ meta-model çš„è¨“ç·´å’Œæ¸¬è©¦é æ¸¬çµæœ
base_model_train_pred = np.zeros((X_train.shape[0], len(base_models)))
base_model_test_pred = np.zeros((X_test.shape[0], len(base_models)))
```

ä½¿ç”¨ oof ä¾†å»ºç«‹ new features

```python
kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=0)
for i, model in enumerate(base_models):
    for train_idx, valid_idx in kf.split(X_train, y_train):
        X_train_fold, y_train_fold = X_train[train_idx], y_train[train_idx]
        X_valid_fold, y_valid_fold = X_train[valid_idx], y_train[valid_idx]

        model.fit(X_train_fold, y_train_fold)
        base_model_train_pred[valid_idx, i] = model.predict_proba(X_valid_fold)[:, 1]
        base_model_test_pred[:, i] += model.predict_proba(X_test)[:, 1] / n_folds
```

è¨“ç·´ meta model ä¸¦åšå‡ºé æ¸¬ï¼Œé€™é‚Šçœ‹çš„å‡ºä¾†å…¶å†å° meta model åšè¨“ç·´æ™‚æ²’æœ‰ç”¨ k-foldï¼Œè€Œæ˜¯ç›´æ¥å» fit æ•´å€‹ new data feauturesï¼Œå€‹äººè¦ºå¾—å·®ç•°æ€§ä¸å¤§ï¼Œå› ç‚ºé‡å° new data features éƒ½æ˜¯åœ¨æœªçŸ¥æ¨™ç±¤ä¸‹å»åšé æ¸¬ï¼Œå¦‚æœéœ€è¦å†ç–Šä¸€å±¤å‰‡æ‰ä¸€å®šéœ€è¦åš k-foldã€‚

```python
# ä½¿ç”¨åŸºæœ¬æ¨¡å‹çš„é æ¸¬çµæœè¨“ç·´ meta-model
meta_model.fit(base_model_train_pred, y_train)

# ä½¿ç”¨ meta-model é æ¸¬æ¸¬è©¦é›†
pred = meta_model.predict_proba(base_model_test_pred)[:, 1]
logloss = log_loss(y_test, pred)
print('logloss: {:.4f}'.format(logloss))
```

æ•ˆæœä¹Ÿæ˜¯æ»¿ä¸éŒ¯çš„ ğŸ¤’

```python
logloss: 0.0859
```



### æ–°çš„è³‡æ–™è¿‘ä¾†å¦‚ä½•ä½¿ç”¨ stacking

å¦‚æœæ˜¯ kaggle çš„æ¯”è³½ï¼Œé‚£ç›´æ¥å° test data åšé æ¸¬å°±å¥½ï¼Œç”±æ–¼ stacking æ¶æ§‹è¼ƒè¤‡é›œï¼Œé‚£å¯¦å‹™ä¸Šè¦æ€éº¼ä½¿ç”¨å‘¢?

1. å…ˆå°‡è¨“ç·´å¥½çš„ model å„²å­˜æˆ pickle æª”ï¼Œé€™æ¨£ä¸‹æ¬¡è¦é æ¸¬æ™‚å¯ä»¥å‘¼å«å‡ºä¾†ä¸ç”¨å†é‡æ–°è¨“ç·´ã€‚

> å¯¦å‹™ä¸Šé€™ç¨®æ¦‚å¿µå°±åƒæ˜¯æˆ‘å€‘ä¸€èˆ¬åœ¨é›»è…¦ä¸Šå°‡æª”æ¡ˆå­˜æˆ zip æˆ– rar ç„¶å¾Œè¦ä½¿ç”¨æ™‚è§£å£“ç¸®ä¸€æ¨£ï¼Œå› æ­¤ä»»ä½•ç¨‹å¼çš„ç‰©ä»¶éƒ½èƒ½ä¸€æ¨£çš„æ“ä½œï¼Œé€™ç¨®æ–¹å¼å°±å¾ˆåƒåœ¨ Spark ä¸Šå°‡ç‰©ä»¶å­˜æˆ parquet

2. å‘¼å«è¨“ç·´å¥½çš„ base model é æ¸¬æ–°çš„ features

```python
# å‡è¨­æœ‰ä¸€ç­†æ–°çš„è³‡æ–™ new_dataï¼Œshape ç‚º (1, 30)
new_data = np.random.rand(10, 30)

# ä½¿ç”¨å·²è¨“ç·´å¥½çš„ base models é æ¸¬ new_data
base_model_pred = np.zeros((10, len(base_models)))
for i, model in enumerate(base_models):
    base_model_pred[:, i] = model.predict_proba(new_data)[:, 1]
```

3. ä½¿ç”¨å·²è¨“ç·´å¥½çš„ meta model é æ¸¬ new_data

```python
meta_model_pred = meta_model.predict_proba(base_model_pred)[:, 1]
print(meta_model_pred)
```

```python
[0.91237343 0.83224565 0.85927932 0.95671311 0.82796747 0.95660662
 0.86298109 0.81283893 0.84073246 0.85952858]
```



ä¸éåƒæ˜¯ä¹³ç™Œé€™ç¨®è³‡æ–™é‚„æœ‰å‹ä¸€éŒ¯èª¤çš„é€™ç¨®ç‹€æ³ï¼Œæ•…å…¶åœ¨ç®—æ©Ÿç‡åšé æ¸¬æ™‚ï¼Œéœ€è¦å†é¡å¤–èª¿æ•´`é–¥å€¼ï¼ˆthresholdï¼‰`ï¼Œä¸éé€™æ˜¯å¦å¤–çš„èª²é¡Œï¼Œæ•´å€‹ stacking çš„æ¶æ§‹å¤§æ¦‚æ˜¯é€™æ¨£ï¼Œä¸éä¸€èˆ¬å¯¦å‹™ä¸Šä¸å¤ªéœ€è¦ä½¿ç”¨é€™éº¼è¤‡é›œçš„æ¶æ§‹ï¼Œä¸€èˆ¬å‡è¨­æ˜¯è¡ŒéŠ·æˆ–åˆ†é¡å¯èƒ½ç”¨å–®ä¸€çš„ GBDT å°±èƒ½è§£æ±ºäº†ã€‚

å› ç‚º stacking çš„æ¶æ§‹éœ€è¦æ¯”è¼ƒå¤§é‡çš„è¨ˆç®—è³‡æºï¼Œå‡è¨­æœ‰ k æŠ˜ï¼ŒN å€‹ model å°±éœ€è¦è¨ˆç®— k*N æ¬¡çš„è¨“ç·´ï¼Œå› æ­¤ç•¶è³‡æ–™é‡å¾ˆå¤§æ™‚ä¸¦ä¸ä¸€å®šé©åˆç”¨åœ¨å¯¦å‹™ä¸Šã€‚





## Referance

[A Deep Dive into Stacking Ensemble Machine Learning â€” Part I](https://towardsdatascience.com/a-deep-dive-into-stacking-ensemble-machine-learning-part-i-10476b2ade3)

[A Deep Dive into Stacking Ensemble Machine Learning â€” Part II](https://towardsdatascience.com/a-deep-dive-into-stacking-ensemble-machine-learning-part-ii-69bfc0d6e53d)

[5.4 Stackingã€æ–¯å¦ç¦21ç§‹å­£ï¼šå®ç”¨æœºå™¨å­¦ä¹ ä¸­æ–‡ç‰ˆã€‘](https://www.youtube.com/watch?v=muoIYXUooAU)

[Introduction to Ensembling/Stacking in Python](https://www.kaggle.com/code/arthurtok/introduction-to-ensembling-stacking-in-python)

[é¨°è¨Šå»£å‘Šé»æ“Šå¤§è³½ï¼šå°stackingçš„ä¸€äº›åŸºæœ¬ä»‹ç´¹](https://cloud.tencent.com/developer/beta/article/1005304)

