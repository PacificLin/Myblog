---
author: Pacific
categories:
- Machine Learning
- Python
- SQL
- data scientist
date: "2023-06-06"
description: 
image: 
math: true
tags:
- æ‰¾å·¥ä½œ
- Python
- åˆ·é¡Œ
- data scientist
- data analyst
title: Data Scientist æˆ– Data Analyst æ‰¾å·¥ä½œéœ€è¦åˆ·é¡Œå—?
---

## åˆ·é¡Œçš„å¿…è¦æ€§

 DS æˆ– DA ä¸å®Œå…¨æ˜¯å·¥ç¨‹å¸«çš„å°å‘ï¼Œå€‹äººèªç‚ºéœ€è¦æ›´å¤šå°æ–¼çµ±è¨ˆçŸ¥è­˜çš„æ™®éæ€§ç†è§£ï¼ˆä¾‹å¦‚ `Bias-Variance Trade-Off`ï¼‰ã€‚ç”¢æ¥­çš„ domain knowledge ä»¥åŠå•†æ¥­é‚è¼¯çš„åˆ¤æ–·å’Œåˆ†æéƒ½æ˜¯å¾ˆé‡è¦çš„èƒ½åŠ›ï¼ŒDS å¯èƒ½éœ€è¦å¤šä¸€é»å°æ¨¡å‹å’Œçµ±è¨ˆæ–¹æ³•çš„ç†è§£å’Œæ‡‰ç”¨ï¼ŒDA éœ€è¦æ›´å¤šçš„èªªæ•…äº‹èƒ½åŠ›ä»¥åŠè³‡æ–™è¦–è¦ºåŒ–ï¼ˆå»ºç«‹ dashboard æœ‰è¦æœƒ ï¼‰å’Œå•†æ¥­çš„æ•ç¨…åº¦ã€‚

ç›¸å°æ–¼æœ‰ engineer å­—å°¾çš„ä¾‹å¦‚ï¼Œalgorithm engineerï¼ŒML engineer æˆ–æ˜¯ data engineer éœ€è¦æ›´å¤šçš„ç¨‹å¼ solid åŸºç¤ï¼Œç•¢ç«Ÿæœƒéœ€è¦è€ƒé‡ç³»çµ±æ•ˆèƒ½ï¼Œå¾Œç«¯çš„ç©©å®šæ€§ï¼ŒDevOpsï¼Œæ¸¬è©¦ç­‰ç­‰ï¼Œé€™äº› position å¯èƒ½éœ€è¦åˆ·æ¯”è¼ƒå¤šé¡æ¼”ç®—æ³•çš„é¡Œç›®ã€‚

Briefly, DS å’Œ DA  éœ€è¦è¼ƒå¼·çš„`è§£æ±ºå•†æ¥­å•é¡Œçš„èƒ½åŠ›`ï¼Œè€Œ engineer çµå°¾çš„éœ€è¦æ¯”è¼ƒå¼·çš„ç¨‹å¼å»ºæ§‹èƒ½åŠ›ã€‚

ç•¶ç„¶æˆ‘æ²’åšé engineer çµå°¾çš„å•¦ ğŸ˜ ğŸ˜‘



## æ‰€ä»¥åˆ°åº•è¦ä¸è¦åˆ·

æˆ‘ä¹‹å‰æ‰¾å·¥ä½œçš„ç¶“é©—ï¼Œæœƒè€ƒå¯«ç¨‹å¼çš„å¤§æ¦‚ 5-7 æˆå·¦å³ï¼Œå…¶ä¸­å¤§æ¦‚æœ‰ 8 æˆéƒ½æ˜¯è€ƒ SQLï¼Œç¾å ´è§£çš„å’Œå¸¶å›å®¶åšçš„éƒ½æœ‰ï¼Œä¸éè€ƒ python å’Œ R ç¾å ´è§£çš„æ¯”è¼ƒå°‘ã€‚ä¸éæˆ‘ä¹Ÿé‡éç•¶å ´çµ¦ dataï¼Œç„¶å¾Œè¦æˆ‘ç¾å ´åˆ†æå¾Œæ‰¾å‡ºä»–å€‘è¦çš„å•†æ¥­å•é¡Œï¼Œä¸€å°æ™‚çµ¦ä½ å¯«é€™æ¨£ï¼ŒçœŸçš„æ˜¯æ»¿ç¡¬çš„ã€‚

çµè«–æˆ‘è¦ºå¾—å¯ä»¥åˆ·ä¸€ä¸‹å•¦ï¼Œä½†ä»¥ SQL ç‚ºä¸»ï¼ŒPython å’Œ R ç‚ºè¼”ã€‚

ä½†åŒæ™‚ä¹Ÿéœ€è¦ç·´ç¿’ä¸€äº› Non-coding Question lolï¼Œå› ç‚ºä¹Ÿæ»¿å¤šæœƒå•é¡Œæ¨¡å‹åŸç†å’Œçµ±è¨ˆåŸºç¤çš„ã€‚



## åˆ·èµ·ä¾† !

é€™æ¬¡è‚²å¬°é›¢è·å¿«ä¸€å¹´ï¼Œéš¨ç„¶ä¸­é–“éƒ½é‚„æ˜¯æœ‰ç© Kaggle å’Œåšå…¬ç›Šå°ˆæ¡ˆï¼Œä½†éƒ½æ˜¯ç”¨ Pythonï¼Œå®Œå…¨æ²’ç¢° SQLï¼Œä½†é¢è©¦åˆå¹¾ä¹éƒ½è€ƒ SQL ï¼Œæ‰€ä»¥æ—¢ç„¶è¦é–‹å§‹æ‰¾å·¥ä½œäº†ï¼Œé‚£å°±èªçœŸåˆ·ä¸€ä¸‹é¡Œã€‚

ç¶²è·¯ä¸Šè³‡æºå…¶å¯¦æ»¿å¤šçš„ï¼Œä½†å¾ˆå¤šéƒ½è¦ä»˜è²»ï¼Œä¸éå¯ä»¥è€ƒæ…®å…ˆæŠŠå…è²»çš„åˆ·å®Œå†ä¾†çœ‹è¦ä¸è¦ä»˜éŒ¢

çœ‹é Reddit æ¨è–¦å¾Œï¼Œæˆ‘ä¸»è¦æ±ºå®šåˆ·ä»¥ä¸‹å¹¾å€‹ï¼š

[LeetCode](https://leetcode.com/list/e97a9e5m/)

[stratascratch](https://www.stratascratch.com/)

[datalemur](https://datalemur.com/)

ç•¶ç„¶ä¹Ÿå¯ä»¥æ‰¾ Kaggle çš„æ¯”è³½ä¾†ç©ï¼Œä½†é¢è©¦è€ƒç¨‹å¼ä¸æœƒè€ƒåˆ°ä½ å¾ 0 åˆ°å»ºä¸€å€‹å®Œæ•´æ¨¡å‹å•¦ï¼Œé€šå¸¸éƒ½æ˜¯ç”¨å•çš„ï¼Œå› ç‚ºå»ºæ¨¡å…¶å¯¦è¦èªçœŸèª¿æ ¡è¦èŠ±å¾ˆå¤šæ™‚é–“æ¢ç´¢å’Œåè¦†æ¸¬è©¦ï¼Œå…‰ cleaning dataï¼Œ EDA å’Œ feature engineering å°±æœƒèŠ±å¾ˆå¤šæ™‚é–“äº†ã€‚å› æ­¤æˆ‘èªç‚ºåˆ·é¡Œå»ç†Ÿæ‚‰è³‡æ–™è™•ç†çš„ç¨‹åºèƒ½åŠ›é‚„æ˜¯æŠ•è³‡å ±é…¬ç‡æœ€å¤§çš„ã€‚

ä»¥ä¸‹æˆ‘æœƒè¨˜éŒ„ä¸€ä¸‹è‡ªå·±åˆ·çš„éç¨‹ä¸­é‡åˆ°æ»¿æœ‰è¶£çš„é¡Œç›®ï¼Œä¸å¤ªæœƒåˆ†é›£æ˜“åº¦ï¼Œä¸»è¦ä»¥è‡ªå·±å¯èƒ½æ¯”è¼ƒä¸ç†Ÿæˆ–è¦ºå¾—æœ‰è¶£ç‚ºä¸»



## Coding Question

---

### Users By Average Session Time

å…¬å¸ï¼šMeta

é›£åº¦ï¼šMedium

å•é¡Œæ ¸å¿ƒï¼štable mergeï½œèšåˆï¼ˆaggregationï¼‰

å•é¡Œï¼š

> Calculate each user's average session time. A session is defined as the time difference between a page_load and page_exit. For simplicity, assume a user has only 1 session per day and if there are multiple of the same events on that day, consider only the latest page_load and earliest page_exit, with an obvious restriction that load time event should happen before exit time event . Output the user_id and their average session time.



`facebook_web_log`

| user_id | timestamp           | action      |
| ------- | ------------------- | ----------- |
| 0       | 2019-04-25 13:30:15 | page_load   |
| 0       | 2019-04-25 13:30:18 | page_load   |
| 0       | 2019-04-25 13:30:40 | scroll_down |
| 0       | 2019-04-25 13:30:45 | scroll_up   |
| 0       | 2019-04-25 13:31:10 | scroll_down |
| 0       | 2019-04-25 13:31:25 | scroll_down |
| 0       | 2019-04-25 13:31:40 | page_exit   |

è§£æ³•ï¼š

data å¦‚è¡¨æ ¼ï¼Œç°¡å–®ä¾†èªªå°±æ˜¯ä¸€èˆ¬åœ¨æ•¸ä½è»Œè·¡ä¸­çš„ sessionã€‚

> session æŒ‡ç”¨æˆ¶ä¸€èˆ¬åœ¨ç¶²ç«™ä¸Šç™»éŒ„å¾Œåˆ°ç™»å‡ºï¼Œæˆ–æ˜¯æœªå‹•ä½œå¾Œè¶…éä¸€æ®µæ™‚é–“ã€‚é€™å€‹å€é–“ä¸€èˆ¬ä¾†èªªæ˜¯ sessionï¼Œä¸éæœ‰äº›æœƒå®šç¾©ä¸€å€‹ session çš„æœ€é•·æ™‚é–“ï¼Œä¾‹å¦‚ 30 åˆ†é˜ï¼Œé›–ç„¶ä½ é€™ 30 åˆ†é˜éƒ½æ²’ç™»å‡ºï¼Œä½†è¶…é 30 åˆ†é˜å¾Œï¼Œä»–æœƒè¨˜éŒ„ç‚ºä¸‹ä¸€å€‹ session

è€Œä»–é¡Œç›®è¦å®šä¸€å€‹ä½¿ç”¨è€…ä¸€å¤©å°±æ˜¯ä¸€å€‹ sessionï¼Œè€Œè‘—å€‹ session çš„è¨ˆç®—æ–¹å¼ç‚ºæœ€å¾Œä¸€å€‹ log-inï¼Œä½†æœ€å…ˆ log-out çš„å€é–“ï¼Œ

ç•¶ç„¶ log-in çš„æ™‚é–“é»è¦å°æ–¼ log-outã€‚

ç„¶å¾Œè¨ˆç®—æ¯å€‹ç”¨æˆ¶çš„å¹³å‡æ¯å¤© session é•·åº¦

```python
# Import libraries
import pandas as pd
import numpy as np

# Start writing code
facebook_web_log.head()
df = facebook_web_log.copy()

# 1.å…ˆå°‡ action å±¬æ–¼ load å’Œ exit çš„è¡Œç‚º filter å‡ºä¾†
df['timestamp'] = pd.to_datetime(df['timestamp'])
p_loads = df.loc[df['action'] == 'page_load', ['user_id', 'timestamp']]
p_exit = df.loc[df['action'] == 'page_exit', ['user_id', 'timestamp']]

# 2. å°‡å…©å€‹ table åˆä½µï¼Œé‚è¼¯æ˜¯æ¯å€‹ action éƒ½å»å°æ‡‰ä¸ç«¥çš„ exit
sessions = pd.merge(p_loads, p_exit, how = 'left', on = 'user_id', suffixes = ['_load', '_exit'])

# 3. load è¦å°æ–¼ exitï¼Œfilter åˆç†æ¢ä»¶çš„ session
sessions = sessions[sessions['timestamp_load'] < sessions['timestamp_exit']]
sessions['timestamp'] = pd.to_datetime(sessions['timestamp_load'])

#4. æ‰¾å‡ºç•¶æ—¥æœ€å¾Œä¸€æ¬¡ç™»éŒ„é‚„æœ‰æœ€å¿«é›¢é–‹çš„æ™‚é–“é»
sessions = sessions.groupby(['user_id', pd.Grouper(key = 'timestamp', freq = 'D')]).agg({'timestamp_load': 'max', 'timestamp_exit': 'min'}).reset_index()

# 5. è¨ˆç®— durationï¼Œé€™è£¡çš„ duration å°±æ˜¯ session çš„æ™‚é–“
sessions['duration'] = sessions['timestamp_exit'] - sessions['timestamp_load']
sessions['duration'] = sessions['duration'].dt.total_seconds()

# 6.åˆ†çµ„è¨ˆç®—æ¯å€‹äººå¹³å‡æ¯å¤©çš„ session æ™‚é–“
answer = sessions.groupby('user_id')['duration'].agg(lambda x: np.mean(x)).reset_index()
```



---

### Premium vs Freemium

å…¬å¸ï¼šMicrosoft

é›£åº¦ï¼šHard

å•é¡Œæ ¸å¿ƒï¼štable mergeï½œèšåˆï¼ˆaggregationï¼‰

å•é¡Œï¼š

> Find the total number of downloads for paying and non-paying users by date. Include only records where non-paying customers have more downloads than paying customers. The output should be sorted by earliest date first and contain 3 columns date, non-paying downloads, paying downloads.



`ms_user_dimension`

| user_id | acc_id |
| ------- | ------ |
| 1       | 716    |
| 2       | 749    |
| 3       | 713    |
| 4       | 744    |
| 5       | 726    |

`ms_acc_dimension`

| acc_id | paying_customer |
| ------ | --------------- |
| 700    | no              |
| 701    | no              |
| 702    | no              |
| 703    | no              |
| 704    | no              |
| 705    | no              |

`ms_download_facts`

| date                | user_id | downloads |
| ------------------- | ------- | --------- |
| 2020-08-24 00:00:00 | 1       | 6         |
| 2020-08-22 00:00:00 | 2       | 6         |
| 2020-08-18 00:00:00 | 3       | 2         |
| 2020-08-24 00:00:00 | 4       | 4         |
| 2020-08-19 00:00:00 | 5       | 7         |
| 2020-08-21 00:00:00 | 6       | 3         |
| 2020-08-24 00:00:00 | 7       | 1         |

è§£æ³•ï¼š

1. å…ˆå°‡ä¸‰å¼µ table æ ¹æ“š key å€¼ merge 
2. é‡å°æœ‰ä»˜è²»å’Œæ²’ä»˜è²»å¾·çš„ç”¨æˆ¶è¡Œç‚ºç”¢ç”Ÿæ–°çš„ä¸€åˆ—ä¸¦å°‡å…¶æ”¹ç‚º int
3. èšåˆå¾ŒåŠ ç¸½ï¼Œä¸¦å°‡æ²’ä»˜è²»çš„å¤§æ–¼æœ‰ä»˜è²»çš„æ—¥æœŸ filter å‡ºä¾†

```python
# Import your libraries
import pandas as pd

# Start writing code
ms_user_dimension.head()

df = ms_user_dimension.copy()
df2 = ms_download_facts.copy()
df3 = ms_acc_dimension.copy()

df1 = df.merge(df3, how = 'left')
df_fnl = df2.merge(df1, how = 'left')
df_fnl.sort_values('date', ascending = True)

df_fnl['no'] = df_fnl.apply(lambda row: row['downloads'] if row['paying_customer'] == 'no' else 0, axis=1)
df_fnl['yes'] = df_fnl.apply(lambda row: row['downloads'] if row['paying_customer'] == 'yes' else 0, axis=1)

result = df_fnl.groupby('date').agg({'no':'sum', 'yes':'sum'}).reset_index()
result[result['no'] > result['yes']]
```

---



### Host Popularity Rental Prices

å…¬å¸ï¼šAirbnb

é›£åº¦ï¼šHard

å•é¡Œï¼šé›œæ¹Š primary key | èšåˆï¼ˆaggregationï¼‰|

> Youâ€™re given a table of rental property searches by users. The table consists of search results and outputs host information for searchers. Find the minimum, average, maximum rental prices for each hostâ€™s popularity rating. The hostâ€™s popularity rating is defined as below: 0 reviews: New 1 to 5 reviews: Rising 6 to 15 reviews: Trending Up 16 to 40 reviews: Popular more than 40 reviews: Hot
>
> Tip: The `id` column in the table refers to the search ID. You'll need to create your own host_id by concating price, room_type, host_since, zipcode, and number_of_reviews.
>
> Output host popularity rating and their minimum, average and maximum rental prices.



`airbnb_host_searches`

| id      | price  | property_type | room_type       | amenities                                                    | accommodates | bathrooms | bed_type | cancellation_policy | cleaning_fee | city | host_identity_verified | host_response_rate | host_since          | neighbourhood     | number_of_reviews | review_scores_rating | zipcode | bedrooms | beds |
| ------- | ------ | ------------- | --------------- | ------------------------------------------------------------ | ------------ | --------- | -------- | ------------------- | ------------ | ---- | ---------------------- | ------------------ | ------------------- | ----------------- | ----------------- | -------------------- | ------- | -------- | ---- |
| 8284881 | 621.46 | House         | Entire home/apt | {TV,"Cable TV",Internet,"Wireless Internet","Air conditioning",Pool,Kitchen,"Free parking on premises",Gym,"Hot tub","Indoor fireplace",Heating,"Family/kid friendly",Washer,Dryer,"Smoke detector","Carbon monoxide detector","First aid kit","Safety card","Fire extinguisher",Essentials,Shampoo,"24-hour check-in",Hangers,"Hair dryer",Iron,"Laptop friendly workspace"} | 8            | 3         | Real Bed | strict              | TRUE         | LA   | f                      | 100%               | 2016-11-01 00:00:00 | Pacific Palisades | 1                 |                      | 90272   | 4        | 6    |
| 8284882 | 621.46 | House         | Entire home/apt | {TV,"Cable TV",Internet,"Wireless Internet","Air conditioning",Pool,Kitchen,"Free parking on premises",Gym,"Hot tub","Indoor fireplace",Heating,"Family/kid friendly",Washer,Dryer,"Smoke detector","Carbon monoxide detector","First aid kit","Safety card","Fire extinguisher",Essentials,Shampoo,"24-hour check-in",Hangers,"Hair dryer",Iron,"Laptop friendly workspace"} | 8            | 3         | Real Bed | strict              | TRUE         | LA   | f                      | 100%               | 2016-11-01 00:00:00 | Pacific Palisades | 1                 |                      | 90272   | 4        | 6    |
| 9479348 | 598.9  | Apartment     | Entire home/apt | {"Wireless Internet","Air conditioning",Kitchen,Heating,"Smoke detector","Carbon monoxide detector",Essentials,Shampoo,Hangers,Iron,"translation missing: en.hosting_amenity_49","translation missing: en.hosting_amenity_50"} | 7            | 2         | Real Bed | strict              | FALSE        | NYC  | f                      | 100%               | 2017-07-03 00:00:00 | Hell's Kitchen    | 1                 | 60                   | 10036   | 3        |      |

è§£æ³•ï¼š

é€™é¡Œä¸»è¦æ˜¯è¦é‡æ–°å®šç¾© host çš„ primary keyï¼Œå› ç‚º table çµ¦ä½ çš„ key å€¼æ˜¯æ¯æ¬¡æœå°‹çš„ï¼Œæ‰€ä»¥ host æœƒæœ‰é‡è¤‡ï¼Œæ‰€ä»¥è¦é‡æ—¢æœ‰çš„ table ä¸­é›œæ¹Šå‡ºæˆ–æ‹¼å‡ºæ–°çš„ key å€¼ã€‚

å®šç¾©å®Œäº†å¾Œæ ¹æ“šé¡Œç›®æ¢ä»¶é‡å°ä¸åŒçš„ host åšåˆ†é¡ã€‚

```python
# Import your libraries
import pandas as pd
import numpy as np
# Start writing code
airbnb_host_searches.head()

df = airbnb_host_searches.copy()

df['host_id'] = df['price'].map(str) + df['room_type'].map(str) + df['host_since'].map(str) + df['zipcode'].map(str)+ df['number_of_reviews'].map(str)

df = df.drop_duplicates(subset = 'host_id')

def review_rating(num):
    if num == 0:
        return 'New'
    elif num <= 5:
        return 'Rising'
    elif num <= 15:
        return 'Trending Up'
    elif num <= 40:
        return 'Popular'
    else:
        return 'Hot'
        
df['host_popularity'] = df['number_of_reviews'].apply(review_rating)
df = df.groupby('host_popularity').agg(min_price = ('price', min), avg_price = ('price', np.mean), max_price = ('price',max)).reset_index()
```

---



### Marketing Campaign Success [Advanced]

å…¬å¸ï¼šAmazon

é›£åº¦ï¼šHard

å•é¡Œæ ¸å¿ƒï¼šæ¢ä»¶é‚è¼¯ç¯©é¸

å•é¡Œï¼š

> You have a table of in-app purchases by user. Users that make their first in-app purchase are placed in a marketing campaign where they see call-to-actions for more in-app purchases. Find the number of users that made additional in-app purchases due to the success of the marketing campaign.
>
> The marketing campaign doesn't start until one day after the initial in-app purchase so users that only made one or multiple purchases on the first day do not count, nor do we count users that over time purchase only the products they purchased on the first day.



`marketing_campaign`

| user_id | created_at          | product_id | quantity | price |
| ------- | ------------------- | ---------- | -------- | ----- |
| 10      | 2019-01-01 00:00:00 | 101        | 3        | 55    |
| 10      | 2019-01-02 00:00:00 | 119        | 5        | 29    |
| 10      | 2019-03-31 00:00:00 | 111        | 2        | 149   |
| 11      | 2019-01-02 00:00:00 | 105        | 3        | 234   |
| 11      | 2019-03-31 00:00:00 | 120        | 3        | 99    |
| 12      | 2019-01-02 00:00:00 | 112        | 2        | 200   |
| 12      | 2019-03-31 00:00:00 | 110        | 2        | 299   |

è§£æ³•ï¼š

é€™é¡Œé¡Œç›®çœ‹äº†å¥½å¹¾ééƒ½é‚„çœ‹ä¸æ‡‚ä»–è¦è¡ä¸‰å°ï¼Œè§£å‡ºä¾†çš„å€¼å’Œç­”æ¡ˆä¸€ç›´ä¸ä¸€æ¨£ï¼Œåªå¥½å»å·çœ‹è§£ç­”åˆ°åº•é€™é¡Œæƒ³å•å•¥é‚è¼¯

ææ‡‚å¾Œï¼Œå…¶å¯¦æ²’æœ‰å¾ˆé›£ä½†ä¸»è¦è¦èƒ½ç¿»è­¯ä¸¦ç†è§£é¡Œç›®è¦å•å•¥ã€‚

ä¸»è¦å°±æ˜¯è³¼è²·è¡Œç‚ºçš„ä¸‰å€‹æ¢ä»¶ï¼Œæ‰æœ‰é”åˆ°ä»–èªç‚ºæœ‰ `marketing camping` çš„æ¢ä»¶

1. åªæœ‰åœ¨ä¸€å¤©å…§è²·æ±è¥¿ï¼ˆä¸ç¬¦åˆï¼‰
2. åªè²·éä¸€æ¨£ç”¢å“é¡å‹çš„æ±è¥¿ï¼ˆä¸ç¬¦åˆï¼‰
3. åœ¨ç¬¬ä¸€å¤©è²·çš„ç”¢å“ä¸­å¾Œä¾†ä¹Ÿæœ‰è²·ç›¸åŒç”¢å“çš„ï¼ˆä¸ç¬¦åˆï¼‰

å»æ‰ä¸Šé¢ä¸‰å€‹æ¢ä»¶å¾Œï¼Œå°±æ˜¯ç­”æ¡ˆï¼Œç¬¬ä¸‰å€‹æ¢ä»¶å…¶å¯¦å¾ˆé›£å¾é¡Œç›®ä¸­é¦¬ä¸Šè½‰æ›éä¾†ï¼Œè‹±æ–‡æˆ‘å°±çˆ›å“­å“­ğŸ¤®ğŸ¤§



```python
# Import your libraries
import pandas as pd

# Start writing code
marketing_campaign.head()
df = marketing_campaign.copy()
df['date'] = df['created_at'].dt.date

# åªæœ‰åœ¨ä¸€å¤©å…§è²·æ±è¥¿
df['purchase_days'] = df.groupby('user_id')['date'].transform('nunique')
# åªè²·éä¸€æ¨£æ±è¥¿
df['purchase_items'] = df.groupby('user_id')['product_id'].transform('nunique')

# åœ¨ç¬¬ä¸€å¤©è²·éçš„æ±è¥¿å¾Œä¾†åˆè²·
# æ¯å€‹äººåœ¨ç¬¬ä¸€å¤©è²·çš„æ±è¥¿
df['first_product'] = df.groupby('user_id')['created_at'].transform(lambda x: df.loc[x.idxmin(), 'product_id'])
df['date'] =  pd.to_datetime(df['date'])
df['rank'] = df.groupby('user_id')['date'].rank(method = 'dense')
mask_df = df[df['rank'] == 1]
mask_df['user_product'] = mask_df['product_id'].map(str) + mask_df['user_id'].map(str)


mask = ((df['purchase_days'] > 1) & (df['purchase_items'] > 1))

filtered_df = df[df['user_id'].isin(df.loc[mask, 'user_id'])]
filtered_df['user_product'] = filtered_df['product_id'].map(str) +filtered_df['user_id'].map(str)
filtered_df = filtered_df[~filtered_df['user_product'].isin(mask_df['user_product'])]

len(filtered_df['user_id'].unique())
```

---



### Y-on-Y Growth Rate

å…¬å¸ï¼šWayfair

é›£åº¦ï¼šHard

å•é¡Œæ ¸å¿ƒï¼šå­æŸ¥è©¢ | window function

å•é¡Œï¼š

> Assume you are given the table below containing information on user transactions for particular products. Write a query to obtain the [year-on-year growth rate](https://www.fundera.com/blog/year-over-year-growth) for the total spend of each product for each year.
>
> Output the year (in ascending order) partitioned by product id, current year's spend, previous year's spend and year-on-year growth rate (percentage rounded to 2 decimal places).



### `user_transactions`

| transaction_id | product_id | spend   | transaction_date    |
| :------------- | :--------- | :------ | :------------------ |
| 1341           | 123424     | 1500.60 | 12/31/2019 12:00:00 |
| 1423           | 123424     | 1000.20 | 12/31/2020 12:00:00 |
| 1623           | 123424     | 1246.44 | 12/31/2021 12:00:00 |
| 1322           | 123424     | 2145.32 | 12/31/2022 12:00:00 |

è§£æ³•ï¼š

ä¸»è¦è¦å°‹æ‰¾ YoYï¼Œä¸€èˆ¬ä¾†èªªè¦æ‰¾ YoYï¼ŒMoM é€™é¡å‹çš„é¡Œç›®éƒ½æœƒç”¨åˆ° `LAG()` æˆ– `LEAD()`ï¼Œé€™é¡Œä¹Ÿä¸ä¾‹å¤–ã€‚

1. é¦–å…ˆå°±æ˜¯å…ˆå°‡ `YEAR` æŠ½å–å‡ºä¾†å¾Œåˆ†çµ„åŠ ç¸½
2. ç”¨ LAG() åšå‡º YoY çš„å·®ç•°ç„¶å¾Œå–è®ŠåŒ–çš„ç™¾åˆ†æ¯”

```sql
with base AS(
SELECT a.*, EXTRACT(YEAR FROM transaction_date) AS current_year
FROM user_transactions AS a
),
 
base2 AS (
SELECT current_year, product_id, SUM(spend) AS total_sp
FROM base a
GROUP BY 1, 2
ORDER BY current_year
),

base3 AS (
SELECT a.*, LAG(total_sp) OVER(PARTITION BY product_id ORDER BY current_year) AS win_lag
FROM base2 AS a
)

SELECT 
  a.current_year AS year, 
  a.product_id,
  a.total_sp AS curr_year_spend,
  a.win_lag AS prev_year_spend,
  ROUND(((total_sp - win_lag) / (win_lag))* 100, 2) AS yoy_rate
FROM base3 a

```

---



### Top 5 States With 5 Star Businesses

å…¬å¸ï¼šYelp

é›£åº¦ï¼šHard

å•é¡Œæ ¸å¿ƒï¼šrank | æ’åº

å•é¡Œï¼š

> Find the top 5 states with the most 5 star businesses. Output the state name along with the number of 5-star businesses and order records by the number of 5-star businesses in descending order. In case there are ties in the number of businesses, return all the unique states. If two states have the same result, sort them in alphabetical order.



`yelp_business`

| business_id            | name                            | neighborhood | address                   | city      | state | postal_code | latitude | longitude | stars | review_count | is_open | categories                                                   |
| ---------------------- | ------------------------------- | ------------ | ------------------------- | --------- | ----- | ----------- | -------- | --------- | ----- | ------------ | ------- | ------------------------------------------------------------ |
| G5ERFWvPfHy7IDAUYlWL2A | All Colors Mobile Bumper Repair |              | 7137 N 28th Ave           | Phoenix   | AZ    | 85051       | 33.448   | -112.074  | 1     | 4            | 1       | Auto Detailing;Automotive                                    |
| 0jDvRJS-z9zdMgOUXgr6rA | Sunfare                         |              | 811 W Deer Valley Rd      | Phoenix   | AZ    | 85027       | 33.683   | -112.085  | 5     | 27           | 1       | Personal Chefs;Food;Gluten-Free;Food Delivery Services;Event Planning & Services;Restaurants |
| 6HmDqeNNZtHMK0t2glF_gg | Dry Clean Vegas                 | Southeast    | 2550 Windmill Ln, Ste 100 | Las Vegas | NV    | 89123       | 36.042   | -115.118  | 1     | 4            | 1       | Dry Cleaning & Laundry;Laundry Services;Local Services;Dry Cleaning |



è§£æ³•ï¼š

1. æ‰¾å‡º stars å¤§æ–¼äº”é¡†æ˜Ÿçš„ businessï¼Œç„¶å¾Œèšåˆ state count ç¸½æ•¸
2.  å› ç‚ºè¦æ‰¾ TOP 5ï¼Œæ•…éœ€è¦ rank() æ’åºï¼Œé€™é‚Šé¡Œç›®æœ‰èªªåŒæ¨£æ•¸é‡çš„å…¨éƒ¨éƒ½å¯ä»¥ç®—ï¼Œæ•…ä¸èƒ½ç”¨ row_number() å’Œ dens_rank()ï¼Œå…¶ä¸­å·®ç•°å¯ä»¥ google æ¯”è¼ƒä¸€ä¸‹
3. ç„¶å¾Œé‡å°æ•¸é‡å’Œ state æ’åº



`SQL è§£æ³•`

```sql
with base as (
    select state, count(distinct business_id) as cmt
    from yelp_business
    where stars >= 5
    group by 1
    order by cmt desc, state
)

select a.state, a.cmt as n_businesses
from (
    select a.*, rank() over(order by cmt desc) as rnk
    from base as a
) as a
where a.rnk <= 5
order by n_businesses desc, state
```



é€™è£¡å¯ä»¥æä¸€ä¸‹ python åœ¨ rank() å‡½æ•¸è£¡çš„åƒæ•¸ `method`å’Œ SQL æœ‰æ¯”è¼ƒä¸ä¸€æ¨£çš„è®ŠåŒ–ï¼Œ SQL çš„ rank() æœ‰åœ¨ä¹‹å‰çš„æ–‡ç« æéæƒ¹ï¼Œå¯åƒè€ƒ [Window Functions in PySpark](https://www.mydatamafia.com/p/window-functions-in-pyspark/)ï¼Œé€™é‚Šèªªæ˜ä¸€ä¸‹ python çš„å·®ç•°

`average`

```python
grouped_df['cmt'].rank(method='average', ascending = False)
```

| state | cmt  | rnk  |
| ----- | ---- | ---- |
| AZ    | 10   | 1    |
| ON    | 5    | 2    |
| NV    | 4    | 3    |
| IL    | 3    | 5    |
| OH    | 3    | 5    |
| WI    | 3    | 5    |
| EDH   | 2    | 7    |
| BW    | 1    | 8.5  |
| QC    | 1    | 8.5  |

æœƒå°‡åŒåæ¬¡çš„å…ƒç´ å¹³å‡æ’åï¼ŒåŸæœ¬ç‚º 4, 5, 6ï¼Œå¹³å‡å‰‡éƒ½ç‚ºç¬¬ 5 åï¼Œ8, 9 å‰‡å¹³å‡ç‚º 8.5 å

`min`

```python
grouped_df['cmt'].rank(method='average', ascending = False)
```

æœƒå°‡åŒåæ¬¡çš„å…ƒç´ é€šé€šçµ¦äºˆæœ€å‰é¢çš„æ’åï¼ŒåŸæœ¬ç‚º 4, 5, 6ï¼Œå¹³å‡å‰‡éƒ½ç‚ºç¬¬ 4 å

| state | cmt  | rnk  |
| ----- | ---- | ---- |
| AZ    | 10   | 1    |
| ON    | 5    | 2    |
| NV    | 4    | 3    |
| IL    | 3    | 4    |
| OH    | 3    | 4    |
| WI    | 3    | 4    |
| EDH   | 2    | 7    |
| BW    | 1    | 8    |
| QC    | 1    | 8    |

`max`

å°±å’Œ min ç›¸åï¼Œæœƒè¿”å›æœ€å¾Œé¢çš„æ’å

`first`

| state | cmt  | rnk  |
| ----- | ---- | ---- |
| AZ    | 10   | 1    |
| ON    | 5    | 2    |
| NV    | 4    | 3    |
| IL    | 3    | 4    |
| OH    | 3    | 5    |
| WI    | 3    | 6    |
| EDH   | 2    | 7    |
| BW    | 1    | 8    |
| QC    | 1    | 9    |

æœƒå°‡åŒåæ¬¡çš„å…ƒç´ æŒ‰ç…§åŸæœ¬æ’åˆ—çš„é †åºæ’åï¼Œé¡ä¼¼ SQL ä¸­çš„ `row_number()`

`desnse`

| state | cmt  | rnk  |
| ----- | ---- | ---- |
| AZ    | 10   | 1    |
| ON    | 5    | 2    |
| NV    | 4    | 3    |
| IL    | 3    | 4    |
| OH    | 3    | 4    |
| WI    | 3    | 4    |
| EDH   | 2    | 5    |
| BW    | 1    | 6    |
| QC    | 1    | 6    |

å’Œ SQL ä¸­çš„ `DENSE_RANK()`ç›¸åŒï¼Œä¸æœƒè·³éç›¸åŒæ’åæ‰€ä½”æ“šçš„æ’åä½ç½®ã€‚

python è£¡çš„ rank() å°±æ˜¯å°‡ SQL ä¸­çš„ `RANK()`å†ç´°åˆ†ç‚º min, max, averageã€‚

`python è§£æ³•`

```python
# Import your libraries
import pandas as pd

# Start writing code
yelp_business.head()

df = yelp_business.copy()

df = df.loc[df['stars'] >= 5, :]

grouped_df = df.groupby('state').agg({'business_id': 'nunique'}).reset_index()
grouped_df.columns = ['state', 'cmt']
# æ ¹æ“š 'cmt' éæ¸›å’Œ 'state' å‡åºé€²è¡Œæ’åº
sorted_df = grouped_df.sort_values(by=['cmt', 'state'], ascending=[False, True])

sorted_df['rnk'] = grouped_df['cmt'].rank(method='min', ascending = False)
top5 = sorted_df.loc[sorted_df['rnk'] <= 5, :].sort_values(by=['cmt', 'state'], ascending=[False, True])
top5[['state', 'cmt']].rename(columns={'cmt': 'n_businesses'}).reset_index(drop=True)
```

---



é€™ä¸€å‘¨å¤§æ¦‚åˆ·äº† 30-40 é¡Œå·¦å³ï¼Œå¤§éƒ¨åˆ†çš„æ‰‹æ„Ÿå›ä¾†æ»¿å¤šçš„ï¼Œæ‡‰è©²æœƒå†æ‰¾å·¥ä½œçš„è·¯ä¸Šæ¯å¤©ç¶­æŒ 1-2 é¡Œï¼Œä½†å¥½åƒå°±è¦ä»˜è²»äº† QQ

